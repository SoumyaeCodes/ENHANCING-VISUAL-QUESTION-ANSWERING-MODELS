# ENHANCING-VISUAL-QUESTION-ANSWERING-MODELS
ENHANCING VISUAL QUESTION ANSWERING MODELS Study

## Introduction
This repository contains the code and documentation for the Visual Question Answering (VQA) project conducted as part of academic research at Northeastern University. This project explores the integration of advanced machine learning techniques in computer vision and natural language processing to develop systems capable of accurately answering questions based on the content of images.

## Project Overview
The VQA project leverages a multimodal fusion model that integrates text processing via BERT and image processing via Google's Vision Transformer (ViT), enhanced with Microsoft's GIT-base-COCO for generating image captions. This approach aims to improve the accuracy and reliability of VQA systems, addressing challenges such as high variance and overfitting observed in baseline models.

## Key Features
Multimodal Fusion Model: Combines textual and visual data processing to generate accurate answers to visual questions.
Advanced Model Architectures: Utilizes BERT for NLP tasks and ViT for image processing tasks.
Ensemble Techniques: Employs ensemble learning strategies to enhance prediction robustness.
Customizable Input Processing: Includes capabilities for prompt engineering and synonym expansion to refine text inputs.

## Prerequisites
Python 3.8+, Pip package manager, Pytorch

## Authors
Soumyae Tyagi - Initial work and main contributor - tyagi.so@northeastern.edu

## Acknowledgments
Northeastern University
Kaggle for the DAQUAR dataset used in this project
Microsoft for the GIT-base-COCO model
Google for the Vision Transformer model

## Contact
For any queries, you can reach out to Soumyae Tyagi at tyagi.so@northeastern.edu.

Feel free to fork this repository if you find it useful for your research or if you wish to collaborate on further enhancements.
