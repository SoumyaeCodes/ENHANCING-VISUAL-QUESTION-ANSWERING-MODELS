{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3798293,"sourceType":"datasetVersion","datasetId":2264789}],"dockerImageVersionId":30703,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport nltk\nimport torch\nimport requests\nimport numpy as np \nimport pandas as pd \nfrom torch import nn\nfrom PIL import Image\nfrom torch.optim import Adam\nfrom nltk.corpus import wordnet\nfrom datasets import load_dataset\nfrom torch.nn import DataParallel\nfrom IPython.display import display\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import AutoTokenizer\nfrom huggingface_hub import hf_hub_download\nfrom transformers import BertModel, ViTModel\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, ViTFeatureExtractor\nfrom transformers import AutoProcessor, AutoModelForCausalLM\nfrom torchvision.transforms import Compose, Resize, Normalize, ToTensor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T20:40:48.965821Z","iopub.execute_input":"2024-04-22T20:40:48.966570Z","iopub.status.idle":"2024-04-22T20:40:50.755491Z","shell.execute_reply.started":"2024-04-22T20:40:48.966537Z","shell.execute_reply":"2024-04-22T20:40:50.754587Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\n    \"csv\", \n    data_files={\n        \"train\": os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\",\"data_train.csv\"),\n        \"test\": os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\", \"data_eval.csv\")\n    }\n)\nwith open(os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\", \"answer_space.txt\")) as f:\n    answer_space = f.read().splitlines()\ndataset = dataset.map(\n    lambda examples: {\n        'label': [\n            answer_space.index(ans.replace(\" \", \"\").split(\",\")[0]) # Select the 1st answer if multiple answers are provided\n            for ans in examples['answer']\n        ]\n    },\n    batched=True\n)\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:43:20.309458Z","iopub.execute_input":"2024-04-22T20:43:20.310306Z","iopub.status.idle":"2024-04-22T20:43:20.577219Z","shell.execute_reply.started":"2024-04-22T20:43:20.310273Z","shell.execute_reply":"2024-04-22T20:43:20.576237Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer', 'image_id', 'label'],\n        num_rows: 9974\n    })\n    test: Dataset({\n        features: ['question', 'answer', 'image_id', 'label'],\n        num_rows: 2494\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-coco\")\n\ndef get_noun_synonyms(text):\n    words = nltk.word_tokenize(text)\n    tags = nltk.pos_tag(words)\n    synonyms = {}\n    for word, tag in tags:\n        if tag.startswith('NN'):  \n            synsets = wordnet.synsets(word, pos=wordnet.NOUN)\n            syn_list = set()\n            for syn in synsets:\n                for lemma in syn.lemmas():\n                    if lemma.name() != word:  \n                        syn_list.add(lemma.name().replace('_', ' '))\n                        if len(syn_list) >= 5:  \n                            break\n                if len(syn_list) >= 5:\n                    break\n            if syn_list:\n                synonyms[word] = ', '.join(list(syn_list)[:5])\n    return synonyms\ndef generate_caption(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return generated_caption\ndef add_captions_and_synonyms(example):\n    base_path = os.path.join(\"..\", \"input\", \"visual-question-answering-computer-vision-nlp\", \"dataset\", \"images\")\n    image_path = os.path.join(base_path, f\"{example['image_id']}.png\")\n    caption = generate_caption(image_path)\n    synonyms = get_noun_synonyms(caption)\n    example['caption'] = caption\n    example['caption_synonyms'] = synonyms\n    return example\ndataset['train'] = dataset['train'].map(add_captions_and_synonyms)\ndataset['test'] = dataset['test'].map(add_captions_and_synonyms)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:43:02.790499Z","iopub.execute_input":"2024-04-22T20:43:02.791946Z","iopub.status.idle":"2024-04-22T20:43:02.797562Z","shell.execute_reply.started":"2024-04-22T20:43:02.791911Z","shell.execute_reply":"2024-04-22T20:43:02.796550Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:43:26.085014Z","iopub.execute_input":"2024-04-22T20:43:26.085665Z","iopub.status.idle":"2024-04-22T20:43:26.091906Z","shell.execute_reply.started":"2024-04-22T20:43:26.085633Z","shell.execute_reply":"2024-04-22T20:43:26.090896Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer', 'image_id', 'label'],\n        num_rows: 9974\n    })\n    test: Dataset({\n        features: ['question', 'answer', 'image_id', 'label'],\n        num_rows: 2494\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"def load_image(image_id):\n    base_path = os.path.join(\"..\", \"input\", \"visual-question-answering-computer-vision-nlp\", \"dataset\", \"images\")\n    image_path = os.path.join(base_path, f\"{image_id}.png\")\n    image = Image.open(image_path).convert(\"RGB\")\n    return feature_extractor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:43:30.129094Z","iopub.execute_input":"2024-04-22T20:43:30.130126Z","iopub.status.idle":"2024-04-22T20:43:30.136462Z","shell.execute_reply.started":"2024-04-22T20:43:30.130084Z","shell.execute_reply":"2024-04-22T20:43:30.135319Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:43:32.564550Z","iopub.execute_input":"2024-04-22T20:43:32.564968Z","iopub.status.idle":"2024-04-22T20:43:33.051618Z","shell.execute_reply.started":"2024-04-22T20:43:32.564938Z","shell.execute_reply":"2024-04-22T20:43:33.050586Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n    return encoding['input_ids'].squeeze(0), encoding['attention_mask'].squeeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:43:34.404666Z","iopub.execute_input":"2024-04-22T20:43:34.405600Z","iopub.status.idle":"2024-04-22T20:43:34.410688Z","shell.execute_reply.started":"2024-04-22T20:43:34.405567Z","shell.execute_reply":"2024-04-22T20:43:34.409691Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"class VQADataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n    def __len__(self):\n        return len(self.dataset)\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        combined_text = f\"Here's the situation: An image is described as '{item['caption']}'. Given this, can you answer: {item['question']}?\"\n        input_ids, attention_mask = preprocess_text(combined_text)\n        image = load_image(item['image_id'])\n        label = torch.tensor(item['label'])\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'pixel_values': image,\n            'labels': label\n        }","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:12:27.909151Z","iopub.execute_input":"2024-04-22T20:12:27.909897Z","iopub.status.idle":"2024-04-22T20:12:27.916252Z","shell.execute_reply.started":"2024-04-22T20:12:27.909865Z","shell.execute_reply":"2024-04-22T20:12:27.915305Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(VQADataset(dataset['train']), batch_size=32, shuffle=True)\ntest_loader = DataLoader(VQADataset(dataset['test']), batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:12:41.948471Z","iopub.execute_input":"2024-04-22T20:12:41.949185Z","iopub.status.idle":"2024-04-22T20:12:41.954192Z","shell.execute_reply.started":"2024-04-22T20:12:41.949134Z","shell.execute_reply":"2024-04-22T20:12:41.953286Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class VisualQuestionAnsweringModel(nn.Module):\n    def __init__(self, num_labels):\n        super(VisualQuestionAnsweringModel, self).__init__()\n        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n        self.image_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n        self.classifier = nn.Linear(self.text_model.config.hidden_size + self.image_model.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask, pixel_values):\n        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n        image_features = self.image_model(pixel_values=pixel_values).pooler_output\n        combined_features = torch.cat((text_features, image_features), dim=1)\n        logits = self.classifier(combined_features)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:13:56.170933Z","iopub.execute_input":"2024-04-22T20:13:56.171331Z","iopub.status.idle":"2024-04-22T20:13:56.178596Z","shell.execute_reply.started":"2024-04-22T20:13:56.171302Z","shell.execute_reply":"2024-04-22T20:13:56.177500Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"num_labels = len(answer_space)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:15:35.236987Z","iopub.execute_input":"2024-04-22T20:15:35.237375Z","iopub.status.idle":"2024-04-22T20:15:35.242229Z","shell.execute_reply.started":"2024-04-22T20:15:35.237345Z","shell.execute_reply":"2024-04-22T20:15:35.241190Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model = VisualQuestionAnsweringModel(num_labels)\nmodel = DataParallel(model)\nmodel.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:15:50.333016Z","iopub.execute_input":"2024-04-22T20:15:50.333742Z","iopub.status.idle":"2024-04-22T20:15:51.383979Z","shell.execute_reply.started":"2024-04-22T20:15:50.333708Z","shell.execute_reply":"2024-04-22T20:15:51.382728Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): VisualQuestionAnsweringModel(\n    (text_model): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (image_model): ViTModel(\n      (embeddings): ViTEmbeddings(\n        (patch_embeddings): ViTPatchEmbeddings(\n          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (encoder): ViTEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x ViTLayer(\n            (attention): ViTAttention(\n              (attention): ViTSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): ViTSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (intermediate): ViTIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): ViTOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          )\n        )\n      )\n      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (pooler): ViTPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (classifier): Linear(in_features=1536, out_features=582, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = Adam(model.parameters(), lr=5e-5)\ncriterion = CrossEntropyLoss()\nnum_epochs = 100\nmodel.train() \nfor epoch in range(num_epochs):\n    total_loss = 0\n    for batch in train_loader:\n        input_ids = batch['input_ids'].to('cuda')\n        attention_mask = batch['attention_mask'].to('cuda')\n        pixel_values = batch['pixel_values'].to('cuda')\n        labels = batch['labels'].to('cuda')\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, pixel_values)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    overall_loss += total_loss\nprint(f\"Total Loss over all epochs: {overall_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:48:05.361662Z","iopub.execute_input":"2024-04-22T20:48:05.362466Z","iopub.status.idle":"2024-04-22T20:48:05.368232Z","shell.execute_reply.started":"2024-04-22T20:48:05.362436Z","shell.execute_reply":"2024-04-22T20:48:05.367222Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Total Loss over all epochs: 0.003401037582088393\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loader = DataLoader(VQADataset(dataset['test']), batch_size=32, shuffle=False)\nmodel.eval()\ntorch.no_grad()\n\npredictions = []\ntrue_labels = []\n\nwith torch.no_grad(): \n    for batch in test_loader:\n        input_ids = batch['input_ids'].to('cuda')\n        attention_mask = batch['attention_mask'].to('cuda')\n        images = batch['pixel_values'].to('cuda')\n        labels = batch['labels'].to('cuda')\n        outputs = model(input_ids, attention_mask, images)\n        preds = torch.argmax(outputs, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(labels.cpu().numpy())\n        \n        \npredicted_answers = [answer_space[pred] for pred in predictions]\ntrue_answers = [answer_space[label] for label in true_labels]\n\ncorrect_count = sum(p == t for p, t in zip(predicted_answers, true_answers))\ntotal = len(predicted_answers)\naccuracy = correct_count / total\nprint(f\"Accuracy: {accuracy:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:48:36.761049Z","iopub.execute_input":"2024-04-22T20:48:36.761451Z","iopub.status.idle":"2024-04-22T20:48:36.767811Z","shell.execute_reply.started":"2024-04-22T20:48:36.761424Z","shell.execute_reply":"2024-04-22T20:48:36.766724Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Accuracy: 0.21\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}