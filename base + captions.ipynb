{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3798293,"sourceType":"datasetVersion","datasetId":2264789}],"dockerImageVersionId":30703,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport requests\nimport numpy as np \nimport pandas as pd \nfrom torch import nn\nfrom PIL import Image\nfrom torch.optim import Adam\nfrom datasets import load_dataset\nfrom torch.nn import DataParallel\nfrom IPython.display import display\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import AutoTokenizer\nfrom huggingface_hub import hf_hub_download\nfrom transformers import BertModel, ViTModel\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, ViTFeatureExtractor\nfrom transformers import AutoProcessor, AutoModelForCausalLM\nfrom torchvision.transforms import Compose, Resize, Normalize, ToTensor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T20:15:01.820659Z","iopub.execute_input":"2024-04-22T20:15:01.821026Z","iopub.status.idle":"2024-04-22T20:15:01.828157Z","shell.execute_reply.started":"2024-04-22T20:15:01.820998Z","shell.execute_reply":"2024-04-22T20:15:01.827219Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\n    \"csv\", \n    data_files={\n        \"train\": os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\",\"data_train.csv\"),\n        \"test\": os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\", \"data_eval.csv\")\n    }\n)\nwith open(os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\", \"answer_space.txt\")) as f:\n    answer_space = f.read().splitlines()\ndataset = dataset.map(\n    lambda examples: {\n        'label': [\n            answer_space.index(ans.replace(\" \", \"\").split(\",\")[0]) # Select the 1st answer if multiple answers are provided\n            for ans in examples['answer']\n        ]\n    },\n    batched=True\n)\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:05:55.792917Z","iopub.execute_input":"2024-04-22T20:05:55.793920Z","iopub.status.idle":"2024-04-22T20:05:56.396123Z","shell.execute_reply.started":"2024-04-22T20:05:55.793883Z","shell.execute_reply":"2024-04-22T20:05:56.395203Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71681202e2ff4f938150560b8d01697e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"038d03d3e8054c778bc3401d2cd4fbfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9974 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c8884e3b1fe4715a83d6d9074c0c5bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2494 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee3abf6f431c4a12b21be7ed0c90722a"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer', 'image_id', 'label'],\n        num_rows: 9974\n    })\n    test: Dataset({\n        features: ['question', 'answer', 'image_id', 'label'],\n        num_rows: 2494\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-coco\")\ndef generate_caption(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return generated_caption\ndef add_captions(example):\n    base_path = os.path.join(\"..\", \"input\", \"visual-question-answering-computer-vision-nlp\", \"dataset\", \"images\")\n    image_path = os.path.join(base_path, f\"{example['image_id']}.png\")\n    caption = generate_caption(image_path)\n    example['caption'] = caption\n    return example\ndataset['train'] = dataset['train'].map(add_captions)\ndataset['test'] = dataset['test'].map(add_captions)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:09:25.712404Z","iopub.execute_input":"2024-04-22T20:09:25.713189Z","iopub.status.idle":"2024-04-22T20:09:25.719149Z","shell.execute_reply.started":"2024-04-22T20:09:25.713148Z","shell.execute_reply":"2024-04-22T20:09:25.718125Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Loading widget...\nLoading widget... \n","output_type":"stream"}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:10:36.047630Z","iopub.execute_input":"2024-04-22T20:10:36.048004Z","iopub.status.idle":"2024-04-22T20:10:36.053363Z","shell.execute_reply.started":"2024-04-22T20:10:36.047977Z","shell.execute_reply":"2024-04-22T20:10:36.052212Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer', 'image_id', 'label', 'caption'],\n        num_rows: 9974\n    })\n    test: Dataset({\n        features: ['question', 'answer', 'image_id', 'label', 'caption'],\n        num_rows: 2494\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_image(image_id):\n    base_path = os.path.join(\"..\", \"input\", \"visual-question-answering-computer-vision-nlp\", \"dataset\", \"images\")\n    image_path = os.path.join(base_path, f\"{image_id}.png\")\n    image = Image.open(image_path).convert(\"RGB\")\n    return feature_extractor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:11:02.095767Z","iopub.execute_input":"2024-04-22T20:11:02.096783Z","iopub.status.idle":"2024-04-22T20:11:02.103195Z","shell.execute_reply.started":"2024-04-22T20:11:02.096739Z","shell.execute_reply":"2024-04-22T20:11:02.102264Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:11:25.499753Z","iopub.execute_input":"2024-04-22T20:11:25.500417Z","iopub.status.idle":"2024-04-22T20:11:25.742935Z","shell.execute_reply.started":"2024-04-22T20:11:25.500387Z","shell.execute_reply":"2024-04-22T20:11:25.741962Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def preprocess_text(text):\n    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n    return encoding['input_ids'].squeeze(0), encoding['attention_mask'].squeeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:11:22.024667Z","iopub.execute_input":"2024-04-22T20:11:22.025551Z","iopub.status.idle":"2024-04-22T20:11:22.030460Z","shell.execute_reply.started":"2024-04-22T20:11:22.025517Z","shell.execute_reply":"2024-04-22T20:11:22.029333Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class VQADataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n    def __len__(self):\n        return len(self.dataset)\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        combined_text = f\"Question: {item['question']} Caption: {item['caption']}\"\n        input_ids, attention_mask = preprocess_text(combined_text)\n        image = load_image(item['image_id'])\n        label = torch.tensor(item['label'])\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'pixel_values': image,\n            'labels': label\n        }","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:12:27.909151Z","iopub.execute_input":"2024-04-22T20:12:27.909897Z","iopub.status.idle":"2024-04-22T20:12:27.916252Z","shell.execute_reply.started":"2024-04-22T20:12:27.909865Z","shell.execute_reply":"2024-04-22T20:12:27.915305Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(VQADataset(dataset['train']), batch_size=32, shuffle=True)\ntest_loader = DataLoader(VQADataset(dataset['test']), batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:12:41.948471Z","iopub.execute_input":"2024-04-22T20:12:41.949185Z","iopub.status.idle":"2024-04-22T20:12:41.954192Z","shell.execute_reply.started":"2024-04-22T20:12:41.949134Z","shell.execute_reply":"2024-04-22T20:12:41.953286Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class VisualQuestionAnsweringModel(nn.Module):\n    def __init__(self, num_labels):\n        super(VisualQuestionAnsweringModel, self).__init__()\n        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n        self.image_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n        self.classifier = nn.Linear(self.text_model.config.hidden_size + self.image_model.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask, pixel_values):\n        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n        image_features = self.image_model(pixel_values=pixel_values).pooler_output\n        combined_features = torch.cat((text_features, image_features), dim=1)\n        logits = self.classifier(combined_features)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:13:56.170933Z","iopub.execute_input":"2024-04-22T20:13:56.171331Z","iopub.status.idle":"2024-04-22T20:13:56.178596Z","shell.execute_reply.started":"2024-04-22T20:13:56.171302Z","shell.execute_reply":"2024-04-22T20:13:56.177500Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"num_labels = len(answer_space)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:15:35.236987Z","iopub.execute_input":"2024-04-22T20:15:35.237375Z","iopub.status.idle":"2024-04-22T20:15:35.242229Z","shell.execute_reply.started":"2024-04-22T20:15:35.237345Z","shell.execute_reply":"2024-04-22T20:15:35.241190Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"model = VisualQuestionAnsweringModel(num_labels)\nmodel = DataParallel(model)\nmodel.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:15:50.333016Z","iopub.execute_input":"2024-04-22T20:15:50.333742Z","iopub.status.idle":"2024-04-22T20:15:51.383979Z","shell.execute_reply.started":"2024-04-22T20:15:50.333708Z","shell.execute_reply":"2024-04-22T20:15:51.382728Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): VisualQuestionAnsweringModel(\n    (text_model): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (image_model): ViTModel(\n      (embeddings): ViTEmbeddings(\n        (patch_embeddings): ViTPatchEmbeddings(\n          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (encoder): ViTEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x ViTLayer(\n            (attention): ViTAttention(\n              (attention): ViTSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): ViTSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (intermediate): ViTIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): ViTOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          )\n        )\n      )\n      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (pooler): ViTPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (classifier): Linear(in_features=1536, out_features=582, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = Adam(model.parameters(), lr=5e-5)\ncriterion = CrossEntropyLoss()\nnum_epochs = 100\nmodel.train() \nfor epoch in range(num_epochs):\n    total_loss = 0\n    for batch in train_loader:\n        input_ids = batch['input_ids'].to('cuda')\n        attention_mask = batch['attention_mask'].to('cuda')\n        pixel_values = batch['pixel_values'].to('cuda')\n        labels = batch['labels'].to('cuda')\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, pixel_values)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    overall_loss += total_loss\nprint(f\"Total Loss over all epochs: {overall_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:27:49.828622Z","iopub.execute_input":"2024-04-22T20:27:49.829530Z","iopub.status.idle":"2024-04-22T20:27:49.835128Z","shell.execute_reply.started":"2024-04-22T20:27:49.829500Z","shell.execute_reply":"2024-04-22T20:27:49.834228Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Total Loss over all epochs: 0.005919005132454913\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loader = DataLoader(VQADataset(dataset['test']), batch_size=32, shuffle=False)\nmodel.eval()\ntorch.no_grad()\n\npredictions = []\ntrue_labels = []\n\nwith torch.no_grad(): \n    for batch in test_loader:\n        input_ids = batch['input_ids'].to('cuda')\n        attention_mask = batch['attention_mask'].to('cuda')\n        images = batch['pixel_values'].to('cuda')\n        labels = batch['labels'].to('cuda')\n        outputs = model(input_ids, attention_mask, images)\n        preds = torch.argmax(outputs, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(labels.cpu().numpy())\n        \n        \npredicted_answers = [answer_space[pred] for pred in predictions]\ntrue_answers = [answer_space[label] for label in true_labels]\n\ncorrect_count = sum(p == t for p, t in zip(predicted_answers, true_answers))\ntotal = len(predicted_answers)\naccuracy = correct_count / total\nprint(f\"Accuracy: {accuracy:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:29:36.457406Z","iopub.execute_input":"2024-04-22T20:29:36.458131Z","iopub.status.idle":"2024-04-22T20:29:36.464132Z","shell.execute_reply.started":"2024-04-22T20:29:36.458099Z","shell.execute_reply":"2024-04-22T20:29:36.463108Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Accuracy: 0.14\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}