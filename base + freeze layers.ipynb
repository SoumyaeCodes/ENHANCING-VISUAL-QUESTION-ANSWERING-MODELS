{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3798293,"sourceType":"datasetVersion","datasetId":2264789}],"dockerImageVersionId":30703,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport requests\nimport numpy as np \nimport pandas as pd \nfrom torch import nn\nfrom PIL import Image\nfrom torch.optim import Adam\nfrom datasets import load_dataset\nfrom torch.nn import DataParallel\nfrom IPython.display import display\nfrom torch.nn import CrossEntropyLoss\nfrom transformers import AutoTokenizer\nfrom huggingface_hub import hf_hub_download\nfrom transformers import BertModel, ViTModel\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, ViTFeatureExtractor\nfrom transformers import AutoProcessor, AutoModelForCausalLM\nfrom torchvision.transforms import Compose, Resize, Normalize, ToTensor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T22:06:16.804729Z","iopub.execute_input":"2024-04-22T22:06:16.805538Z","iopub.status.idle":"2024-04-22T22:06:45.905006Z","shell.execute_reply.started":"2024-04-22T22:06:16.805506Z","shell.execute_reply":"2024-04-22T22:06:45.904184Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-22 22:06:32.841526: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-22 22:06:32.841627: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-22 22:06:33.113433: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = load_dataset(\n    \"csv\", \n    data_files={\n        \"train\": os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\",\"data_train.csv\"),\n        \"test\": os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\", \"data_eval.csv\")\n    }\n)\nwith open(os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\", \"answer_space.txt\")) as f:\n    answer_space = f.read().splitlines()\ndataset = dataset.map(\n    lambda examples: {\n        'label': [\n            answer_space.index(ans.replace(\" \", \"\").split(\",\")[0]) # Select the 1st answer if multiple answers are provided\n            for ans in examples['answer']\n        ]\n    },\n    batched=True\n)\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:06:48.882735Z","iopub.execute_input":"2024-04-22T22:06:48.883773Z","iopub.status.idle":"2024-04-22T22:06:49.469535Z","shell.execute_reply.started":"2024-04-22T22:06:48.883743Z","shell.execute_reply":"2024-04-22T22:06:49.468639Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cf8146d05994c6f930001af31f5a996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e37cc3ece8e44d18a063214ab9425d96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9974 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9440e16555844c4da2f469b1b6156323"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2494 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe33c7591779491393d95c885c1ec7f1"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer', 'image_id', 'label'],\n        num_rows: 9974\n    })\n    test: Dataset({\n        features: ['question', 'answer', 'image_id', 'label'],\n        num_rows: 2494\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"def load_image(image_id):\n    base_path = os.path.join(\"..\", \"input\", \"visual-question-answering-computer-vision-nlp\", \"dataset\", \"images\")\n    image_path = os.path.join(base_path, f\"{image_id}.png\")\n    image = Image.open(image_path).convert(\"RGB\")\n    return feature_extractor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:07:14.591777Z","iopub.execute_input":"2024-04-22T22:07:14.592690Z","iopub.status.idle":"2024-04-22T22:07:14.597993Z","shell.execute_reply.started":"2024-04-22T22:07:14.592655Z","shell.execute_reply":"2024-04-22T22:07:14.596958Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:07:22.320383Z","iopub.execute_input":"2024-04-22T22:07:22.320718Z","iopub.status.idle":"2024-04-22T22:07:23.648386Z","shell.execute_reply.started":"2024-04-22T22:07:22.320694Z","shell.execute_reply":"2024-04-22T22:07:23.647557Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"375c14c5dc6f40aba4f66c58cc8f445f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cc72d55eb3f4b3d9a7e0a829f2ba814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e00f4092e2f143e090b08e54410c836d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8284096a064542e4adc3dac08ca2bbaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef2a71057c7347cbb866ced77cc7015c"}},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_text(text):\n    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n    return encoding['input_ids'].squeeze(0), encoding['attention_mask'].squeeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:07:29.845313Z","iopub.execute_input":"2024-04-22T22:07:29.845670Z","iopub.status.idle":"2024-04-22T22:07:29.850821Z","shell.execute_reply.started":"2024-04-22T22:07:29.845642Z","shell.execute_reply":"2024-04-22T22:07:29.849923Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class VQADataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n    def __len__(self):\n        return len(self.dataset)\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        combined_text = f\"Here's the situation: An image is described as '{item['caption']}'. Given this, can you answer: {item['question']}?\"\n        input_ids, attention_mask = preprocess_text(combined_text)\n        image = load_image(item['image_id'])\n        label = torch.tensor(item['label'])\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'pixel_values': image,\n            'labels': label\n        }","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:07:37.618995Z","iopub.execute_input":"2024-04-22T22:07:37.619367Z","iopub.status.idle":"2024-04-22T22:07:37.628242Z","shell.execute_reply.started":"2024-04-22T22:07:37.619336Z","shell.execute_reply":"2024-04-22T22:07:37.627289Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(VQADataset(dataset['train']), batch_size=32, shuffle=True)\ntest_loader = DataLoader(VQADataset(dataset['test']), batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:07:44.552832Z","iopub.execute_input":"2024-04-22T22:07:44.553209Z","iopub.status.idle":"2024-04-22T22:07:44.558587Z","shell.execute_reply.started":"2024-04-22T22:07:44.553182Z","shell.execute_reply":"2024-04-22T22:07:44.557543Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class VisualQuestionAnsweringModel(nn.Module):\n    def __init__(self, num_labels):\n        super(VisualQuestionAnsweringModel, self).__init__()\n        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n        self.image_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n        self.classifier = nn.Linear(self.text_model.config.hidden_size + self.image_model.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask, pixel_values):\n        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n        image_features = self.image_model(pixel_values=pixel_values).pooler_output\n        combined_features = torch.cat((text_features, image_features), dim=1)\n        logits = self.classifier(combined_features)\n        return logits\n    def freeze_layers(self, freeze_fraction=0.7):\n        all_parameters = list(self.text_model.named_parameters()) + list(self.image_model.named_parameters())\n        total_params = len(all_parameters)\n        freeze_until = int(total_params * freeze_fraction)\n        all_parameters.sort(key=lambda x: x[0])\n        for name, param in all_parameters[:freeze_until]:\n            param.requires_grad = False\n        for name, param in all_parameters:\n            print(f\"{name}: {'frozen' if not param.requires_grad else 'not frozen'}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:08:09.689039Z","iopub.execute_input":"2024-04-22T22:08:09.689911Z","iopub.status.idle":"2024-04-22T22:08:09.700018Z","shell.execute_reply.started":"2024-04-22T22:08:09.689877Z","shell.execute_reply":"2024-04-22T22:08:09.699122Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"num_labels = len(answer_space)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:08:19.839830Z","iopub.execute_input":"2024-04-22T22:08:19.840619Z","iopub.status.idle":"2024-04-22T22:08:19.844563Z","shell.execute_reply.started":"2024-04-22T22:08:19.840590Z","shell.execute_reply":"2024-04-22T22:08:19.843585Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model = VisualQuestionAnsweringModel(num_labels)\nmodel.freeze_layers()\nmodel = DataParallel(model)\nmodel.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:08:28.778641Z","iopub.execute_input":"2024-04-22T22:08:28.779529Z","iopub.status.idle":"2024-04-22T22:08:34.341696Z","shell.execute_reply.started":"2024-04-22T22:08:28.779486Z","shell.execute_reply":"2024-04-22T22:08:34.340815Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9db575927e24f2eb4c4ffbfaa0cef7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48c1d1ae02be4ef68dfe8c57a20a6a49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"546eaaf8b61d45dd92f8a2c8fdb0f925"}},"metadata":{}},{"name":"stdout","text":"embeddings.LayerNorm.bias: frozen\nembeddings.LayerNorm.weight: frozen\nembeddings.cls_token: frozen\nembeddings.patch_embeddings.projection.bias: frozen\nembeddings.patch_embeddings.projection.weight: frozen\nembeddings.position_embeddings: frozen\nembeddings.position_embeddings.weight: frozen\nembeddings.token_type_embeddings.weight: frozen\nembeddings.word_embeddings.weight: frozen\nencoder.layer.0.attention.attention.key.bias: frozen\nencoder.layer.0.attention.attention.key.weight: frozen\nencoder.layer.0.attention.attention.query.bias: frozen\nencoder.layer.0.attention.attention.query.weight: frozen\nencoder.layer.0.attention.attention.value.bias: frozen\nencoder.layer.0.attention.attention.value.weight: frozen\nencoder.layer.0.attention.output.LayerNorm.bias: frozen\nencoder.layer.0.attention.output.LayerNorm.weight: frozen\nencoder.layer.0.attention.output.dense.bias: frozen\nencoder.layer.0.attention.output.dense.bias: frozen\nencoder.layer.0.attention.output.dense.weight: frozen\nencoder.layer.0.attention.output.dense.weight: frozen\nencoder.layer.0.attention.self.key.bias: frozen\nencoder.layer.0.attention.self.key.weight: frozen\nencoder.layer.0.attention.self.query.bias: frozen\nencoder.layer.0.attention.self.query.weight: frozen\nencoder.layer.0.attention.self.value.bias: frozen\nencoder.layer.0.attention.self.value.weight: frozen\nencoder.layer.0.intermediate.dense.bias: frozen\nencoder.layer.0.intermediate.dense.bias: frozen\nencoder.layer.0.intermediate.dense.weight: frozen\nencoder.layer.0.intermediate.dense.weight: frozen\nencoder.layer.0.layernorm_after.bias: frozen\nencoder.layer.0.layernorm_after.weight: frozen\nencoder.layer.0.layernorm_before.bias: frozen\nencoder.layer.0.layernorm_before.weight: frozen\nencoder.layer.0.output.LayerNorm.bias: frozen\nencoder.layer.0.output.LayerNorm.weight: frozen\nencoder.layer.0.output.dense.bias: frozen\nencoder.layer.0.output.dense.bias: frozen\nencoder.layer.0.output.dense.weight: frozen\nencoder.layer.0.output.dense.weight: frozen\nencoder.layer.1.attention.attention.key.bias: frozen\nencoder.layer.1.attention.attention.key.weight: frozen\nencoder.layer.1.attention.attention.query.bias: frozen\nencoder.layer.1.attention.attention.query.weight: frozen\nencoder.layer.1.attention.attention.value.bias: frozen\nencoder.layer.1.attention.attention.value.weight: frozen\nencoder.layer.1.attention.output.LayerNorm.bias: frozen\nencoder.layer.1.attention.output.LayerNorm.weight: frozen\nencoder.layer.1.attention.output.dense.bias: frozen\nencoder.layer.1.attention.output.dense.bias: frozen\nencoder.layer.1.attention.output.dense.weight: frozen\nencoder.layer.1.attention.output.dense.weight: frozen\nencoder.layer.1.attention.self.key.bias: frozen\nencoder.layer.1.attention.self.key.weight: frozen\nencoder.layer.1.attention.self.query.bias: frozen\nencoder.layer.1.attention.self.query.weight: frozen\nencoder.layer.1.attention.self.value.bias: frozen\nencoder.layer.1.attention.self.value.weight: frozen\nencoder.layer.1.intermediate.dense.bias: frozen\nencoder.layer.1.intermediate.dense.bias: frozen\nencoder.layer.1.intermediate.dense.weight: frozen\nencoder.layer.1.intermediate.dense.weight: frozen\nencoder.layer.1.layernorm_after.bias: frozen\nencoder.layer.1.layernorm_after.weight: frozen\nencoder.layer.1.layernorm_before.bias: frozen\nencoder.layer.1.layernorm_before.weight: frozen\nencoder.layer.1.output.LayerNorm.bias: frozen\nencoder.layer.1.output.LayerNorm.weight: frozen\nencoder.layer.1.output.dense.bias: frozen\nencoder.layer.1.output.dense.bias: frozen\nencoder.layer.1.output.dense.weight: frozen\nencoder.layer.1.output.dense.weight: frozen\nencoder.layer.10.attention.attention.key.bias: frozen\nencoder.layer.10.attention.attention.key.weight: frozen\nencoder.layer.10.attention.attention.query.bias: frozen\nencoder.layer.10.attention.attention.query.weight: frozen\nencoder.layer.10.attention.attention.value.bias: frozen\nencoder.layer.10.attention.attention.value.weight: frozen\nencoder.layer.10.attention.output.LayerNorm.bias: frozen\nencoder.layer.10.attention.output.LayerNorm.weight: frozen\nencoder.layer.10.attention.output.dense.bias: frozen\nencoder.layer.10.attention.output.dense.bias: frozen\nencoder.layer.10.attention.output.dense.weight: frozen\nencoder.layer.10.attention.output.dense.weight: frozen\nencoder.layer.10.attention.self.key.bias: frozen\nencoder.layer.10.attention.self.key.weight: frozen\nencoder.layer.10.attention.self.query.bias: frozen\nencoder.layer.10.attention.self.query.weight: frozen\nencoder.layer.10.attention.self.value.bias: frozen\nencoder.layer.10.attention.self.value.weight: frozen\nencoder.layer.10.intermediate.dense.bias: frozen\nencoder.layer.10.intermediate.dense.bias: frozen\nencoder.layer.10.intermediate.dense.weight: frozen\nencoder.layer.10.intermediate.dense.weight: frozen\nencoder.layer.10.layernorm_after.bias: frozen\nencoder.layer.10.layernorm_after.weight: frozen\nencoder.layer.10.layernorm_before.bias: frozen\nencoder.layer.10.layernorm_before.weight: frozen\nencoder.layer.10.output.LayerNorm.bias: frozen\nencoder.layer.10.output.LayerNorm.weight: frozen\nencoder.layer.10.output.dense.bias: frozen\nencoder.layer.10.output.dense.bias: frozen\nencoder.layer.10.output.dense.weight: frozen\nencoder.layer.10.output.dense.weight: frozen\nencoder.layer.11.attention.attention.key.bias: frozen\nencoder.layer.11.attention.attention.key.weight: frozen\nencoder.layer.11.attention.attention.query.bias: frozen\nencoder.layer.11.attention.attention.query.weight: frozen\nencoder.layer.11.attention.attention.value.bias: frozen\nencoder.layer.11.attention.attention.value.weight: frozen\nencoder.layer.11.attention.output.LayerNorm.bias: frozen\nencoder.layer.11.attention.output.LayerNorm.weight: frozen\nencoder.layer.11.attention.output.dense.bias: frozen\nencoder.layer.11.attention.output.dense.bias: frozen\nencoder.layer.11.attention.output.dense.weight: frozen\nencoder.layer.11.attention.output.dense.weight: frozen\nencoder.layer.11.attention.self.key.bias: frozen\nencoder.layer.11.attention.self.key.weight: frozen\nencoder.layer.11.attention.self.query.bias: frozen\nencoder.layer.11.attention.self.query.weight: frozen\nencoder.layer.11.attention.self.value.bias: frozen\nencoder.layer.11.attention.self.value.weight: frozen\nencoder.layer.11.intermediate.dense.bias: frozen\nencoder.layer.11.intermediate.dense.bias: frozen\nencoder.layer.11.intermediate.dense.weight: frozen\nencoder.layer.11.intermediate.dense.weight: frozen\nencoder.layer.11.layernorm_after.bias: frozen\nencoder.layer.11.layernorm_after.weight: frozen\nencoder.layer.11.layernorm_before.bias: frozen\nencoder.layer.11.layernorm_before.weight: frozen\nencoder.layer.11.output.LayerNorm.bias: frozen\nencoder.layer.11.output.LayerNorm.weight: frozen\nencoder.layer.11.output.dense.bias: frozen\nencoder.layer.11.output.dense.bias: frozen\nencoder.layer.11.output.dense.weight: frozen\nencoder.layer.11.output.dense.weight: frozen\nencoder.layer.2.attention.attention.key.bias: frozen\nencoder.layer.2.attention.attention.key.weight: frozen\nencoder.layer.2.attention.attention.query.bias: frozen\nencoder.layer.2.attention.attention.query.weight: frozen\nencoder.layer.2.attention.attention.value.bias: frozen\nencoder.layer.2.attention.attention.value.weight: frozen\nencoder.layer.2.attention.output.LayerNorm.bias: frozen\nencoder.layer.2.attention.output.LayerNorm.weight: frozen\nencoder.layer.2.attention.output.dense.bias: frozen\nencoder.layer.2.attention.output.dense.bias: frozen\nencoder.layer.2.attention.output.dense.weight: frozen\nencoder.layer.2.attention.output.dense.weight: frozen\nencoder.layer.2.attention.self.key.bias: frozen\nencoder.layer.2.attention.self.key.weight: frozen\nencoder.layer.2.attention.self.query.bias: frozen\nencoder.layer.2.attention.self.query.weight: frozen\nencoder.layer.2.attention.self.value.bias: frozen\nencoder.layer.2.attention.self.value.weight: frozen\nencoder.layer.2.intermediate.dense.bias: frozen\nencoder.layer.2.intermediate.dense.bias: frozen\nencoder.layer.2.intermediate.dense.weight: frozen\nencoder.layer.2.intermediate.dense.weight: frozen\nencoder.layer.2.layernorm_after.bias: frozen\nencoder.layer.2.layernorm_after.weight: frozen\nencoder.layer.2.layernorm_before.bias: frozen\nencoder.layer.2.layernorm_before.weight: frozen\nencoder.layer.2.output.LayerNorm.bias: frozen\nencoder.layer.2.output.LayerNorm.weight: frozen\nencoder.layer.2.output.dense.bias: frozen\nencoder.layer.2.output.dense.bias: frozen\nencoder.layer.2.output.dense.weight: frozen\nencoder.layer.2.output.dense.weight: frozen\nencoder.layer.3.attention.attention.key.bias: frozen\nencoder.layer.3.attention.attention.key.weight: frozen\nencoder.layer.3.attention.attention.query.bias: frozen\nencoder.layer.3.attention.attention.query.weight: frozen\nencoder.layer.3.attention.attention.value.bias: frozen\nencoder.layer.3.attention.attention.value.weight: frozen\nencoder.layer.3.attention.output.LayerNorm.bias: frozen\nencoder.layer.3.attention.output.LayerNorm.weight: frozen\nencoder.layer.3.attention.output.dense.bias: frozen\nencoder.layer.3.attention.output.dense.bias: frozen\nencoder.layer.3.attention.output.dense.weight: frozen\nencoder.layer.3.attention.output.dense.weight: frozen\nencoder.layer.3.attention.self.key.bias: frozen\nencoder.layer.3.attention.self.key.weight: frozen\nencoder.layer.3.attention.self.query.bias: frozen\nencoder.layer.3.attention.self.query.weight: frozen\nencoder.layer.3.attention.self.value.bias: frozen\nencoder.layer.3.attention.self.value.weight: frozen\nencoder.layer.3.intermediate.dense.bias: frozen\nencoder.layer.3.intermediate.dense.bias: frozen\nencoder.layer.3.intermediate.dense.weight: frozen\nencoder.layer.3.intermediate.dense.weight: frozen\nencoder.layer.3.layernorm_after.bias: frozen\nencoder.layer.3.layernorm_after.weight: frozen\nencoder.layer.3.layernorm_before.bias: frozen\nencoder.layer.3.layernorm_before.weight: frozen\nencoder.layer.3.output.LayerNorm.bias: frozen\nencoder.layer.3.output.LayerNorm.weight: frozen\nencoder.layer.3.output.dense.bias: frozen\nencoder.layer.3.output.dense.bias: frozen\nencoder.layer.3.output.dense.weight: frozen\nencoder.layer.3.output.dense.weight: frozen\nencoder.layer.4.attention.attention.key.bias: frozen\nencoder.layer.4.attention.attention.key.weight: frozen\nencoder.layer.4.attention.attention.query.bias: frozen\nencoder.layer.4.attention.attention.query.weight: frozen\nencoder.layer.4.attention.attention.value.bias: frozen\nencoder.layer.4.attention.attention.value.weight: frozen\nencoder.layer.4.attention.output.LayerNorm.bias: frozen\nencoder.layer.4.attention.output.LayerNorm.weight: frozen\nencoder.layer.4.attention.output.dense.bias: frozen\nencoder.layer.4.attention.output.dense.bias: frozen\nencoder.layer.4.attention.output.dense.weight: frozen\nencoder.layer.4.attention.output.dense.weight: frozen\nencoder.layer.4.attention.self.key.bias: frozen\nencoder.layer.4.attention.self.key.weight: frozen\nencoder.layer.4.attention.self.query.bias: frozen\nencoder.layer.4.attention.self.query.weight: frozen\nencoder.layer.4.attention.self.value.bias: frozen\nencoder.layer.4.attention.self.value.weight: frozen\nencoder.layer.4.intermediate.dense.bias: frozen\nencoder.layer.4.intermediate.dense.bias: frozen\nencoder.layer.4.intermediate.dense.weight: frozen\nencoder.layer.4.intermediate.dense.weight: frozen\nencoder.layer.4.layernorm_after.bias: frozen\nencoder.layer.4.layernorm_after.weight: frozen\nencoder.layer.4.layernorm_before.bias: frozen\nencoder.layer.4.layernorm_before.weight: frozen\nencoder.layer.4.output.LayerNorm.bias: frozen\nencoder.layer.4.output.LayerNorm.weight: frozen\nencoder.layer.4.output.dense.bias: frozen\nencoder.layer.4.output.dense.bias: frozen\nencoder.layer.4.output.dense.weight: frozen\nencoder.layer.4.output.dense.weight: frozen\nencoder.layer.5.attention.attention.key.bias: frozen\nencoder.layer.5.attention.attention.key.weight: frozen\nencoder.layer.5.attention.attention.query.bias: frozen\nencoder.layer.5.attention.attention.query.weight: frozen\nencoder.layer.5.attention.attention.value.bias: frozen\nencoder.layer.5.attention.attention.value.weight: frozen\nencoder.layer.5.attention.output.LayerNorm.bias: frozen\nencoder.layer.5.attention.output.LayerNorm.weight: frozen\nencoder.layer.5.attention.output.dense.bias: frozen\nencoder.layer.5.attention.output.dense.bias: frozen\nencoder.layer.5.attention.output.dense.weight: frozen\nencoder.layer.5.attention.output.dense.weight: frozen\nencoder.layer.5.attention.self.key.bias: frozen\nencoder.layer.5.attention.self.key.weight: frozen\nencoder.layer.5.attention.self.query.bias: frozen\nencoder.layer.5.attention.self.query.weight: frozen\nencoder.layer.5.attention.self.value.bias: frozen\nencoder.layer.5.attention.self.value.weight: frozen\nencoder.layer.5.intermediate.dense.bias: frozen\nencoder.layer.5.intermediate.dense.bias: frozen\nencoder.layer.5.intermediate.dense.weight: frozen\nencoder.layer.5.intermediate.dense.weight: frozen\nencoder.layer.5.layernorm_after.bias: frozen\nencoder.layer.5.layernorm_after.weight: frozen\nencoder.layer.5.layernorm_before.bias: frozen\nencoder.layer.5.layernorm_before.weight: frozen\nencoder.layer.5.output.LayerNorm.bias: frozen\nencoder.layer.5.output.LayerNorm.weight: frozen\nencoder.layer.5.output.dense.bias: frozen\nencoder.layer.5.output.dense.bias: frozen\nencoder.layer.5.output.dense.weight: frozen\nencoder.layer.5.output.dense.weight: frozen\nencoder.layer.6.attention.attention.key.bias: frozen\nencoder.layer.6.attention.attention.key.weight: frozen\nencoder.layer.6.attention.attention.query.bias: frozen\nencoder.layer.6.attention.attention.query.weight: frozen\nencoder.layer.6.attention.attention.value.bias: frozen\nencoder.layer.6.attention.attention.value.weight: frozen\nencoder.layer.6.attention.output.LayerNorm.bias: frozen\nencoder.layer.6.attention.output.LayerNorm.weight: frozen\nencoder.layer.6.attention.output.dense.bias: frozen\nencoder.layer.6.attention.output.dense.bias: frozen\nencoder.layer.6.attention.output.dense.weight: frozen\nencoder.layer.6.attention.output.dense.weight: frozen\nencoder.layer.6.attention.self.key.bias: frozen\nencoder.layer.6.attention.self.key.weight: frozen\nencoder.layer.6.attention.self.query.bias: not frozen\nencoder.layer.6.attention.self.query.weight: not frozen\nencoder.layer.6.attention.self.value.bias: not frozen\nencoder.layer.6.attention.self.value.weight: not frozen\nencoder.layer.6.intermediate.dense.bias: not frozen\nencoder.layer.6.intermediate.dense.bias: not frozen\nencoder.layer.6.intermediate.dense.weight: not frozen\nencoder.layer.6.intermediate.dense.weight: not frozen\nencoder.layer.6.layernorm_after.bias: not frozen\nencoder.layer.6.layernorm_after.weight: not frozen\nencoder.layer.6.layernorm_before.bias: not frozen\nencoder.layer.6.layernorm_before.weight: not frozen\nencoder.layer.6.output.LayerNorm.bias: not frozen\nencoder.layer.6.output.LayerNorm.weight: not frozen\nencoder.layer.6.output.dense.bias: not frozen\nencoder.layer.6.output.dense.bias: not frozen\nencoder.layer.6.output.dense.weight: not frozen\nencoder.layer.6.output.dense.weight: not frozen\nencoder.layer.7.attention.attention.key.bias: not frozen\nencoder.layer.7.attention.attention.key.weight: not frozen\nencoder.layer.7.attention.attention.query.bias: not frozen\nencoder.layer.7.attention.attention.query.weight: not frozen\nencoder.layer.7.attention.attention.value.bias: not frozen\nencoder.layer.7.attention.attention.value.weight: not frozen\nencoder.layer.7.attention.output.LayerNorm.bias: not frozen\nencoder.layer.7.attention.output.LayerNorm.weight: not frozen\nencoder.layer.7.attention.output.dense.bias: not frozen\nencoder.layer.7.attention.output.dense.bias: not frozen\nencoder.layer.7.attention.output.dense.weight: not frozen\nencoder.layer.7.attention.output.dense.weight: not frozen\nencoder.layer.7.attention.self.key.bias: not frozen\nencoder.layer.7.attention.self.key.weight: not frozen\nencoder.layer.7.attention.self.query.bias: not frozen\nencoder.layer.7.attention.self.query.weight: not frozen\nencoder.layer.7.attention.self.value.bias: not frozen\nencoder.layer.7.attention.self.value.weight: not frozen\nencoder.layer.7.intermediate.dense.bias: not frozen\nencoder.layer.7.intermediate.dense.bias: not frozen\nencoder.layer.7.intermediate.dense.weight: not frozen\nencoder.layer.7.intermediate.dense.weight: not frozen\nencoder.layer.7.layernorm_after.bias: not frozen\nencoder.layer.7.layernorm_after.weight: not frozen\nencoder.layer.7.layernorm_before.bias: not frozen\nencoder.layer.7.layernorm_before.weight: not frozen\nencoder.layer.7.output.LayerNorm.bias: not frozen\nencoder.layer.7.output.LayerNorm.weight: not frozen\nencoder.layer.7.output.dense.bias: not frozen\nencoder.layer.7.output.dense.bias: not frozen\nencoder.layer.7.output.dense.weight: not frozen\nencoder.layer.7.output.dense.weight: not frozen\nencoder.layer.8.attention.attention.key.bias: not frozen\nencoder.layer.8.attention.attention.key.weight: not frozen\nencoder.layer.8.attention.attention.query.bias: not frozen\nencoder.layer.8.attention.attention.query.weight: not frozen\nencoder.layer.8.attention.attention.value.bias: not frozen\nencoder.layer.8.attention.attention.value.weight: not frozen\nencoder.layer.8.attention.output.LayerNorm.bias: not frozen\nencoder.layer.8.attention.output.LayerNorm.weight: not frozen\nencoder.layer.8.attention.output.dense.bias: not frozen\nencoder.layer.8.attention.output.dense.bias: not frozen\nencoder.layer.8.attention.output.dense.weight: not frozen\nencoder.layer.8.attention.output.dense.weight: not frozen\nencoder.layer.8.attention.self.key.bias: not frozen\nencoder.layer.8.attention.self.key.weight: not frozen\nencoder.layer.8.attention.self.query.bias: not frozen\nencoder.layer.8.attention.self.query.weight: not frozen\nencoder.layer.8.attention.self.value.bias: not frozen\nencoder.layer.8.attention.self.value.weight: not frozen\nencoder.layer.8.intermediate.dense.bias: not frozen\nencoder.layer.8.intermediate.dense.bias: not frozen\nencoder.layer.8.intermediate.dense.weight: not frozen\nencoder.layer.8.intermediate.dense.weight: not frozen\nencoder.layer.8.layernorm_after.bias: not frozen\nencoder.layer.8.layernorm_after.weight: not frozen\nencoder.layer.8.layernorm_before.bias: not frozen\nencoder.layer.8.layernorm_before.weight: not frozen\nencoder.layer.8.output.LayerNorm.bias: not frozen\nencoder.layer.8.output.LayerNorm.weight: not frozen\nencoder.layer.8.output.dense.bias: not frozen\nencoder.layer.8.output.dense.bias: not frozen\nencoder.layer.8.output.dense.weight: not frozen\nencoder.layer.8.output.dense.weight: not frozen\nencoder.layer.9.attention.attention.key.bias: not frozen\nencoder.layer.9.attention.attention.key.weight: not frozen\nencoder.layer.9.attention.attention.query.bias: not frozen\nencoder.layer.9.attention.attention.query.weight: not frozen\nencoder.layer.9.attention.attention.value.bias: not frozen\nencoder.layer.9.attention.attention.value.weight: not frozen\nencoder.layer.9.attention.output.LayerNorm.bias: not frozen\nencoder.layer.9.attention.output.LayerNorm.weight: not frozen\nencoder.layer.9.attention.output.dense.bias: not frozen\nencoder.layer.9.attention.output.dense.bias: not frozen\nencoder.layer.9.attention.output.dense.weight: not frozen\nencoder.layer.9.attention.output.dense.weight: not frozen\nencoder.layer.9.attention.self.key.bias: not frozen\nencoder.layer.9.attention.self.key.weight: not frozen\nencoder.layer.9.attention.self.query.bias: not frozen\nencoder.layer.9.attention.self.query.weight: not frozen\nencoder.layer.9.attention.self.value.bias: not frozen\nencoder.layer.9.attention.self.value.weight: not frozen\nencoder.layer.9.intermediate.dense.bias: not frozen\nencoder.layer.9.intermediate.dense.bias: not frozen\nencoder.layer.9.intermediate.dense.weight: not frozen\nencoder.layer.9.intermediate.dense.weight: not frozen\nencoder.layer.9.layernorm_after.bias: not frozen\nencoder.layer.9.layernorm_after.weight: not frozen\nencoder.layer.9.layernorm_before.bias: not frozen\nencoder.layer.9.layernorm_before.weight: not frozen\nencoder.layer.9.output.LayerNorm.bias: not frozen\nencoder.layer.9.output.LayerNorm.weight: not frozen\nencoder.layer.9.output.dense.bias: not frozen\nencoder.layer.9.output.dense.bias: not frozen\nencoder.layer.9.output.dense.weight: not frozen\nencoder.layer.9.output.dense.weight: not frozen\nlayernorm.bias: not frozen\nlayernorm.weight: not frozen\npooler.dense.bias: not frozen\npooler.dense.bias: not frozen\npooler.dense.weight: not frozen\npooler.dense.weight: not frozen\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): VisualQuestionAnsweringModel(\n    (text_model): BertModel(\n      (embeddings): BertEmbeddings(\n        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (encoder): BertEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BertLayer(\n            (attention): BertAttention(\n              (self): BertSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (output): BertSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n            (intermediate): BertIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BertOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n      )\n      (pooler): BertPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (image_model): ViTModel(\n      (embeddings): ViTEmbeddings(\n        (patch_embeddings): ViTPatchEmbeddings(\n          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n        )\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (encoder): ViTEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x ViTLayer(\n            (attention): ViTAttention(\n              (attention): ViTSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): ViTSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (intermediate): ViTIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): ViTOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          )\n        )\n      )\n      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (pooler): ViTPooler(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (activation): Tanh()\n      )\n    )\n    (classifier): Linear(in_features=1536, out_features=582, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = Adam(model.parameters(), lr=5e-5)\ncriterion = CrossEntropyLoss()\nnum_epochs = 100\nmodel.train() \nfor epoch in range(num_epochs):\n    total_loss = 0\n    for batch in train_loader:\n        input_ids = batch['input_ids'].to('cuda')\n        attention_mask = batch['attention_mask'].to('cuda')\n        pixel_values = batch['pixel_values'].to('cuda')\n        labels = batch['labels'].to('cuda')\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, pixel_values)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    overall_loss += total_loss\nprint(f\"Total Loss over all epochs: {overall_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:10:51.029151Z","iopub.execute_input":"2024-04-22T22:10:51.029760Z","iopub.status.idle":"2024-04-22T22:10:51.035196Z","shell.execute_reply.started":"2024-04-22T22:10:51.029730Z","shell.execute_reply":"2024-04-22T22:10:51.034186Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Total Loss over all epochs: 0.009664024118755535\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loader = DataLoader(VQADataset(dataset['test']), batch_size=32, shuffle=False)\nmodel.eval()\ntorch.no_grad()\n\npredictions = []\ntrue_labels = []\n\nwith torch.no_grad(): \n    for batch in test_loader:\n        input_ids = batch['input_ids'].to('cuda')\n        attention_mask = batch['attention_mask'].to('cuda')\n        images = batch['pixel_values'].to('cuda')\n        labels = batch['labels'].to('cuda')\n        outputs = model(input_ids, attention_mask, images)\n        preds = torch.argmax(outputs, dim=1)\n        predictions.extend(preds.cpu().numpy())\n        true_labels.extend(labels.cpu().numpy())\n        \n        \npredicted_answers = [answer_space[pred] for pred in predictions]\ntrue_answers = [answer_space[label] for label in true_labels]\n\ncorrect_count = sum(p == t for p, t in zip(predicted_answers, true_answers))\ntotal = len(predicted_answers)\naccuracy = correct_count / total\nprint(f\"Accuracy: {accuracy:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:15:07.984575Z","iopub.execute_input":"2024-04-22T22:15:07.985572Z","iopub.status.idle":"2024-04-22T22:15:07.991390Z","shell.execute_reply.started":"2024-04-22T22:15:07.985535Z","shell.execute_reply":"2024-04-22T22:15:07.990410Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Accuracy: 0.13\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}