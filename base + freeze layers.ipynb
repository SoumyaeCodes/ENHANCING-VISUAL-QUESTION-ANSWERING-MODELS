{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-22T22:06:16.805538Z","iopub.status.busy":"2024-04-22T22:06:16.804729Z","iopub.status.idle":"2024-04-22T22:06:45.905006Z","shell.execute_reply":"2024-04-22T22:06:45.904184Z","shell.execute_reply.started":"2024-04-22T22:06:16.805506Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-22 22:06:32.841526: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-22 22:06:32.841627: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-22 22:06:33.113433: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import os\n","import torch\n","import requests\n","import numpy as np \n","import pandas as pd \n","from torch import nn\n","from PIL import Image\n","from torch.optim import Adam\n","from datasets import load_dataset\n","from torch.nn import DataParallel\n","from IPython.display import display\n","from torch.nn import CrossEntropyLoss\n","from transformers import AutoTokenizer\n","from huggingface_hub import hf_hub_download\n","from transformers import BertModel, ViTModel\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, ViTFeatureExtractor\n","from transformers import AutoProcessor, AutoModelForCausalLM\n","from torchvision.transforms import Compose, Resize, Normalize, ToTensor\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T22:06:48.883773Z","iopub.status.busy":"2024-04-22T22:06:48.882735Z","iopub.status.idle":"2024-04-22T22:06:49.469535Z","shell.execute_reply":"2024-04-22T22:06:49.468639Z","shell.execute_reply.started":"2024-04-22T22:06:48.883743Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4cf8146d05994c6f930001af31f5a996","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e37cc3ece8e44d18a063214ab9425d96","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9440e16555844c4da2f469b1b6156323","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/9974 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe33c7591779491393d95c885c1ec7f1","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/2494 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['question', 'answer', 'image_id', 'label'],\n","        num_rows: 9974\n","    })\n","    test: Dataset({\n","        features: ['question', 'answer', 'image_id', 'label'],\n","        num_rows: 2494\n","    })\n","})"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["dataset = load_dataset(\n","    \"csv\", \n","    data_files={\n","        \"train\": os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\",\"data_train.csv\"),\n","        \"test\": os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\", \"data_eval.csv\")\n","    }\n",")\n","with open(os.path.join(\"..\",\"input\",\"visual-question-answering-computer-vision-nlp\",\"dataset\", \"answer_space.txt\")) as f:\n","    answer_space = f.read().splitlines()\n","dataset = dataset.map(\n","    lambda examples: {\n","        'label': [\n","            answer_space.index(ans.replace(\" \", \"\").split(\",\")[0]) # Select the 1st answer if multiple answers are provided\n","            for ans in examples['answer']\n","        ]\n","    },\n","    batched=True\n",")\n","dataset"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T22:07:14.592690Z","iopub.status.busy":"2024-04-22T22:07:14.591777Z","iopub.status.idle":"2024-04-22T22:07:14.597993Z","shell.execute_reply":"2024-04-22T22:07:14.596958Z","shell.execute_reply.started":"2024-04-22T22:07:14.592655Z"},"trusted":true},"outputs":[],"source":["def load_image(image_id):\n","    base_path = os.path.join(\"..\", \"input\", \"visual-question-answering-computer-vision-nlp\", \"dataset\", \"images\")\n","    image_path = os.path.join(base_path, f\"{image_id}.png\")\n","    image = Image.open(image_path).convert(\"RGB\")\n","    return feature_extractor(image, return_tensors=\"pt\")['pixel_values'].squeeze(0)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T22:07:22.320718Z","iopub.status.busy":"2024-04-22T22:07:22.320383Z","iopub.status.idle":"2024-04-22T22:07:23.648386Z","shell.execute_reply":"2024-04-22T22:07:23.647557Z","shell.execute_reply.started":"2024-04-22T22:07:22.320694Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"375c14c5dc6f40aba4f66c58cc8f445f","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7cc72d55eb3f4b3d9a7e0a829f2ba814","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e00f4092e2f143e090b08e54410c836d","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8284096a064542e4adc3dac08ca2bbaa","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ef2a71057c7347cbb866ced77cc7015c","version_major":2,"version_minor":0},"text/plain":["preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T22:07:29.845670Z","iopub.status.busy":"2024-04-22T22:07:29.845313Z","iopub.status.idle":"2024-04-22T22:07:29.850821Z","shell.execute_reply":"2024-04-22T22:07:29.849923Z","shell.execute_reply.started":"2024-04-22T22:07:29.845642Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text):\n","    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n","    return encoding['input_ids'].squeeze(0), encoding['attention_mask'].squeeze(0)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T22:07:37.619367Z","iopub.status.busy":"2024-04-22T22:07:37.618995Z","iopub.status.idle":"2024-04-22T22:07:37.628242Z","shell.execute_reply":"2024-04-22T22:07:37.627289Z","shell.execute_reply.started":"2024-04-22T22:07:37.619336Z"},"trusted":true},"outputs":[],"source":["class VQADataset(Dataset):\n","    def __init__(self, dataset):\n","        self.dataset = dataset\n","    def __len__(self):\n","        return len(self.dataset)\n","    def __getitem__(self, idx):\n","        item = self.dataset[idx]\n","        combined_text = f\"Question: {item['question']}\"\n","        input_ids, attention_mask = preprocess_text(combined_text)\n","        image = load_image(item['image_id'])\n","        label = torch.tensor(item['label'])\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'pixel_values': image,\n","            'labels': label\n","        }"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T22:07:44.553209Z","iopub.status.busy":"2024-04-22T22:07:44.552832Z","iopub.status.idle":"2024-04-22T22:07:44.558587Z","shell.execute_reply":"2024-04-22T22:07:44.557543Z","shell.execute_reply.started":"2024-04-22T22:07:44.553182Z"},"trusted":true},"outputs":[],"source":["train_loader = DataLoader(VQADataset(dataset['train']), batch_size=32, shuffle=True)\n","test_loader = DataLoader(VQADataset(dataset['test']), batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T22:08:09.689911Z","iopub.status.busy":"2024-04-22T22:08:09.689039Z","iopub.status.idle":"2024-04-22T22:08:09.700018Z","shell.execute_reply":"2024-04-22T22:08:09.699122Z","shell.execute_reply.started":"2024-04-22T22:08:09.689877Z"},"trusted":true},"outputs":[],"source":["class VisualQuestionAnsweringModel(nn.Module):\n","    def __init__(self, num_labels):\n","        super(VisualQuestionAnsweringModel, self).__init__()\n","        self.text_model = BertModel.from_pretrained('bert-base-uncased')\n","        self.image_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","        self.classifier = nn.Linear(self.text_model.config.hidden_size + self.image_model.config.hidden_size, num_labels)\n","\n","    def forward(self, input_ids, attention_mask, pixel_values):\n","        text_features = self.text_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n","        image_features = self.image_model(pixel_values=pixel_values).pooler_output\n","        combined_features = torch.cat((text_features, image_features), dim=1)\n","        logits = self.classifier(combined_features)\n","        return logits\n","    def freeze_layers(self, freeze_fraction=0.7):\n","        all_parameters = list(self.text_model.named_parameters()) + list(self.image_model.named_parameters())\n","        total_params = len(all_parameters)\n","        freeze_until = int(total_params * freeze_fraction)\n","        all_parameters.sort(key=lambda x: x[0])\n","        for name, param in all_parameters[:freeze_until]:\n","            param.requires_grad = False\n","        for name, param in all_parameters:\n","            print(f\"{name}: {'frozen' if not param.requires_grad else 'not frozen'}\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T22:08:19.840619Z","iopub.status.busy":"2024-04-22T22:08:19.839830Z","iopub.status.idle":"2024-04-22T22:08:19.844563Z","shell.execute_reply":"2024-04-22T22:08:19.843585Z","shell.execute_reply.started":"2024-04-22T22:08:19.840590Z"},"trusted":true},"outputs":[],"source":["num_labels = len(answer_space)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T22:08:28.779529Z","iopub.status.busy":"2024-04-22T22:08:28.778641Z","iopub.status.idle":"2024-04-22T22:08:34.341696Z","shell.execute_reply":"2024-04-22T22:08:34.340815Z","shell.execute_reply.started":"2024-04-22T22:08:28.779486Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a9db575927e24f2eb4c4ffbfaa0cef7a","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48c1d1ae02be4ef68dfe8c57a20a6a49","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"546eaaf8b61d45dd92f8a2c8fdb0f925","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["embeddings.LayerNorm.bias: frozen\n","embeddings.LayerNorm.weight: frozen\n","embeddings.cls_token: frozen\n","embeddings.patch_embeddings.projection.bias: frozen\n","embeddings.patch_embeddings.projection.weight: frozen\n","embeddings.position_embeddings: frozen\n","embeddings.position_embeddings.weight: frozen\n","embeddings.token_type_embeddings.weight: frozen\n","embeddings.word_embeddings.weight: frozen\n","encoder.layer.0.attention.attention.key.bias: frozen\n","encoder.layer.0.attention.attention.key.weight: frozen\n","encoder.layer.0.attention.attention.query.bias: frozen\n","encoder.layer.0.attention.attention.query.weight: frozen\n","encoder.layer.0.attention.attention.value.bias: frozen\n","encoder.layer.0.attention.attention.value.weight: frozen\n","encoder.layer.0.attention.output.LayerNorm.bias: frozen\n","encoder.layer.0.attention.output.LayerNorm.weight: frozen\n","encoder.layer.0.attention.output.dense.bias: frozen\n","encoder.layer.0.attention.output.dense.bias: frozen\n","encoder.layer.0.attention.output.dense.weight: frozen\n","encoder.layer.0.attention.output.dense.weight: frozen\n","encoder.layer.0.attention.self.key.bias: frozen\n","encoder.layer.0.attention.self.key.weight: frozen\n","encoder.layer.0.attention.self.query.bias: frozen\n","encoder.layer.0.attention.self.query.weight: frozen\n","encoder.layer.0.attention.self.value.bias: frozen\n","encoder.layer.0.attention.self.value.weight: frozen\n","encoder.layer.0.intermediate.dense.bias: frozen\n","encoder.layer.0.intermediate.dense.bias: frozen\n","encoder.layer.0.intermediate.dense.weight: frozen\n","encoder.layer.0.intermediate.dense.weight: frozen\n","encoder.layer.0.layernorm_after.bias: frozen\n","encoder.layer.0.layernorm_after.weight: frozen\n","encoder.layer.0.layernorm_before.bias: frozen\n","encoder.layer.0.layernorm_before.weight: frozen\n","encoder.layer.0.output.LayerNorm.bias: frozen\n","encoder.layer.0.output.LayerNorm.weight: frozen\n","encoder.layer.0.output.dense.bias: frozen\n","encoder.layer.0.output.dense.bias: frozen\n","encoder.layer.0.output.dense.weight: frozen\n","encoder.layer.0.output.dense.weight: frozen\n","encoder.layer.1.attention.attention.key.bias: frozen\n","encoder.layer.1.attention.attention.key.weight: frozen\n","encoder.layer.1.attention.attention.query.bias: frozen\n","encoder.layer.1.attention.attention.query.weight: frozen\n","encoder.layer.1.attention.attention.value.bias: frozen\n","encoder.layer.1.attention.attention.value.weight: frozen\n","encoder.layer.1.attention.output.LayerNorm.bias: frozen\n","encoder.layer.1.attention.output.LayerNorm.weight: frozen\n","encoder.layer.1.attention.output.dense.bias: frozen\n","encoder.layer.1.attention.output.dense.bias: frozen\n","encoder.layer.1.attention.output.dense.weight: frozen\n","encoder.layer.1.attention.output.dense.weight: frozen\n","encoder.layer.1.attention.self.key.bias: frozen\n","encoder.layer.1.attention.self.key.weight: frozen\n","encoder.layer.1.attention.self.query.bias: frozen\n","encoder.layer.1.attention.self.query.weight: frozen\n","encoder.layer.1.attention.self.value.bias: frozen\n","encoder.layer.1.attention.self.value.weight: frozen\n","encoder.layer.1.intermediate.dense.bias: frozen\n","encoder.layer.1.intermediate.dense.bias: frozen\n","encoder.layer.1.intermediate.dense.weight: frozen\n","encoder.layer.1.intermediate.dense.weight: frozen\n","encoder.layer.1.layernorm_after.bias: frozen\n","encoder.layer.1.layernorm_after.weight: frozen\n","encoder.layer.1.layernorm_before.bias: frozen\n","encoder.layer.1.layernorm_before.weight: frozen\n","encoder.layer.1.output.LayerNorm.bias: frozen\n","encoder.layer.1.output.LayerNorm.weight: frozen\n","encoder.layer.1.output.dense.bias: frozen\n","encoder.layer.1.output.dense.bias: frozen\n","encoder.layer.1.output.dense.weight: frozen\n","encoder.layer.1.output.dense.weight: frozen\n","encoder.layer.10.attention.attention.key.bias: frozen\n","encoder.layer.10.attention.attention.key.weight: frozen\n","encoder.layer.10.attention.attention.query.bias: frozen\n","encoder.layer.10.attention.attention.query.weight: frozen\n","encoder.layer.10.attention.attention.value.bias: frozen\n","encoder.layer.10.attention.attention.value.weight: frozen\n","encoder.layer.10.attention.output.LayerNorm.bias: frozen\n","encoder.layer.10.attention.output.LayerNorm.weight: frozen\n","encoder.layer.10.attention.output.dense.bias: frozen\n","encoder.layer.10.attention.output.dense.bias: frozen\n","encoder.layer.10.attention.output.dense.weight: frozen\n","encoder.layer.10.attention.output.dense.weight: frozen\n","encoder.layer.10.attention.self.key.bias: frozen\n","encoder.layer.10.attention.self.key.weight: frozen\n","encoder.layer.10.attention.self.query.bias: frozen\n","encoder.layer.10.attention.self.query.weight: frozen\n","encoder.layer.10.attention.self.value.bias: frozen\n","encoder.layer.10.attention.self.value.weight: frozen\n","encoder.layer.10.intermediate.dense.bias: frozen\n","encoder.layer.10.intermediate.dense.bias: frozen\n","encoder.layer.10.intermediate.dense.weight: frozen\n","encoder.layer.10.intermediate.dense.weight: frozen\n","encoder.layer.10.layernorm_after.bias: frozen\n","encoder.layer.10.layernorm_after.weight: frozen\n","encoder.layer.10.layernorm_before.bias: frozen\n","encoder.layer.10.layernorm_before.weight: frozen\n","encoder.layer.10.output.LayerNorm.bias: frozen\n","encoder.layer.10.output.LayerNorm.weight: frozen\n","encoder.layer.10.output.dense.bias: frozen\n","encoder.layer.10.output.dense.bias: frozen\n","encoder.layer.10.output.dense.weight: frozen\n","encoder.layer.10.output.dense.weight: frozen\n","encoder.layer.11.attention.attention.key.bias: frozen\n","encoder.layer.11.attention.attention.key.weight: frozen\n","encoder.layer.11.attention.attention.query.bias: frozen\n","encoder.layer.11.attention.attention.query.weight: frozen\n","encoder.layer.11.attention.attention.value.bias: frozen\n","encoder.layer.11.attention.attention.value.weight: frozen\n","encoder.layer.11.attention.output.LayerNorm.bias: frozen\n","encoder.layer.11.attention.output.LayerNorm.weight: frozen\n","encoder.layer.11.attention.output.dense.bias: frozen\n","encoder.layer.11.attention.output.dense.bias: frozen\n","encoder.layer.11.attention.output.dense.weight: frozen\n","encoder.layer.11.attention.output.dense.weight: frozen\n","encoder.layer.11.attention.self.key.bias: frozen\n","encoder.layer.11.attention.self.key.weight: frozen\n","encoder.layer.11.attention.self.query.bias: frozen\n","encoder.layer.11.attention.self.query.weight: frozen\n","encoder.layer.11.attention.self.value.bias: frozen\n","encoder.layer.11.attention.self.value.weight: frozen\n","encoder.layer.11.intermediate.dense.bias: frozen\n","encoder.layer.11.intermediate.dense.bias: frozen\n","encoder.layer.11.intermediate.dense.weight: frozen\n","encoder.layer.11.intermediate.dense.weight: frozen\n","encoder.layer.11.layernorm_after.bias: frozen\n","encoder.layer.11.layernorm_after.weight: frozen\n","encoder.layer.11.layernorm_before.bias: frozen\n","encoder.layer.11.layernorm_before.weight: frozen\n","encoder.layer.11.output.LayerNorm.bias: frozen\n","encoder.layer.11.output.LayerNorm.weight: frozen\n","encoder.layer.11.output.dense.bias: frozen\n","encoder.layer.11.output.dense.bias: frozen\n","encoder.layer.11.output.dense.weight: frozen\n","encoder.layer.11.output.dense.weight: frozen\n","encoder.layer.2.attention.attention.key.bias: frozen\n","encoder.layer.2.attention.attention.key.weight: frozen\n","encoder.layer.2.attention.attention.query.bias: frozen\n","encoder.layer.2.attention.attention.query.weight: frozen\n","encoder.layer.2.attention.attention.value.bias: frozen\n","encoder.layer.2.attention.attention.value.weight: frozen\n","encoder.layer.2.attention.output.LayerNorm.bias: frozen\n","encoder.layer.2.attention.output.LayerNorm.weight: frozen\n","encoder.layer.2.attention.output.dense.bias: frozen\n","encoder.layer.2.attention.output.dense.bias: frozen\n","encoder.layer.2.attention.output.dense.weight: frozen\n","encoder.layer.2.attention.output.dense.weight: frozen\n","encoder.layer.2.attention.self.key.bias: frozen\n","encoder.layer.2.attention.self.key.weight: frozen\n","encoder.layer.2.attention.self.query.bias: frozen\n","encoder.layer.2.attention.self.query.weight: frozen\n","encoder.layer.2.attention.self.value.bias: frozen\n","encoder.layer.2.attention.self.value.weight: frozen\n","encoder.layer.2.intermediate.dense.bias: frozen\n","encoder.layer.2.intermediate.dense.bias: frozen\n","encoder.layer.2.intermediate.dense.weight: frozen\n","encoder.layer.2.intermediate.dense.weight: frozen\n","encoder.layer.2.layernorm_after.bias: frozen\n","encoder.layer.2.layernorm_after.weight: frozen\n","encoder.layer.2.layernorm_before.bias: frozen\n","encoder.layer.2.layernorm_before.weight: frozen\n","encoder.layer.2.output.LayerNorm.bias: frozen\n","encoder.layer.2.output.LayerNorm.weight: frozen\n","encoder.layer.2.output.dense.bias: frozen\n","encoder.layer.2.output.dense.bias: frozen\n","encoder.layer.2.output.dense.weight: frozen\n","encoder.layer.2.output.dense.weight: frozen\n","encoder.layer.3.attention.attention.key.bias: frozen\n","encoder.layer.3.attention.attention.key.weight: frozen\n","encoder.layer.3.attention.attention.query.bias: frozen\n","encoder.layer.3.attention.attention.query.weight: frozen\n","encoder.layer.3.attention.attention.value.bias: frozen\n","encoder.layer.3.attention.attention.value.weight: frozen\n","encoder.layer.3.attention.output.LayerNorm.bias: frozen\n","encoder.layer.3.attention.output.LayerNorm.weight: frozen\n","encoder.layer.3.attention.output.dense.bias: frozen\n","encoder.layer.3.attention.output.dense.bias: frozen\n","encoder.layer.3.attention.output.dense.weight: frozen\n","encoder.layer.3.attention.output.dense.weight: frozen\n","encoder.layer.3.attention.self.key.bias: frozen\n","encoder.layer.3.attention.self.key.weight: frozen\n","encoder.layer.3.attention.self.query.bias: frozen\n","encoder.layer.3.attention.self.query.weight: frozen\n","encoder.layer.3.attention.self.value.bias: frozen\n","encoder.layer.3.attention.self.value.weight: frozen\n","encoder.layer.3.intermediate.dense.bias: frozen\n","encoder.layer.3.intermediate.dense.bias: frozen\n","encoder.layer.3.intermediate.dense.weight: frozen\n","encoder.layer.3.intermediate.dense.weight: frozen\n","encoder.layer.3.layernorm_after.bias: frozen\n","encoder.layer.3.layernorm_after.weight: frozen\n","encoder.layer.3.layernorm_before.bias: frozen\n","encoder.layer.3.layernorm_before.weight: frozen\n","encoder.layer.3.output.LayerNorm.bias: frozen\n","encoder.layer.3.output.LayerNorm.weight: frozen\n","encoder.layer.3.output.dense.bias: frozen\n","encoder.layer.3.output.dense.bias: frozen\n","encoder.layer.3.output.dense.weight: frozen\n","encoder.layer.3.output.dense.weight: frozen\n","encoder.layer.4.attention.attention.key.bias: frozen\n","encoder.layer.4.attention.attention.key.weight: frozen\n","encoder.layer.4.attention.attention.query.bias: frozen\n","encoder.layer.4.attention.attention.query.weight: frozen\n","encoder.layer.4.attention.attention.value.bias: frozen\n","encoder.layer.4.attention.attention.value.weight: frozen\n","encoder.layer.4.attention.output.LayerNorm.bias: frozen\n","encoder.layer.4.attention.output.LayerNorm.weight: frozen\n","encoder.layer.4.attention.output.dense.bias: frozen\n","encoder.layer.4.attention.output.dense.bias: frozen\n","encoder.layer.4.attention.output.dense.weight: frozen\n","encoder.layer.4.attention.output.dense.weight: frozen\n","encoder.layer.4.attention.self.key.bias: frozen\n","encoder.layer.4.attention.self.key.weight: frozen\n","encoder.layer.4.attention.self.query.bias: frozen\n","encoder.layer.4.attention.self.query.weight: frozen\n","encoder.layer.4.attention.self.value.bias: frozen\n","encoder.layer.4.attention.self.value.weight: frozen\n","encoder.layer.4.intermediate.dense.bias: frozen\n","encoder.layer.4.intermediate.dense.bias: frozen\n","encoder.layer.4.intermediate.dense.weight: frozen\n","encoder.layer.4.intermediate.dense.weight: frozen\n","encoder.layer.4.layernorm_after.bias: frozen\n","encoder.layer.4.layernorm_after.weight: frozen\n","encoder.layer.4.layernorm_before.bias: frozen\n","encoder.layer.4.layernorm_before.weight: frozen\n","encoder.layer.4.output.LayerNorm.bias: frozen\n","encoder.layer.4.output.LayerNorm.weight: frozen\n","encoder.layer.4.output.dense.bias: frozen\n","encoder.layer.4.output.dense.bias: frozen\n","encoder.layer.4.output.dense.weight: frozen\n","encoder.layer.4.output.dense.weight: frozen\n","encoder.layer.5.attention.attention.key.bias: frozen\n","encoder.layer.5.attention.attention.key.weight: frozen\n","encoder.layer.5.attention.attention.query.bias: frozen\n","encoder.layer.5.attention.attention.query.weight: frozen\n","encoder.layer.5.attention.attention.value.bias: frozen\n","encoder.layer.5.attention.attention.value.weight: frozen\n","encoder.layer.5.attention.output.LayerNorm.bias: frozen\n","encoder.layer.5.attention.output.LayerNorm.weight: frozen\n","encoder.layer.5.attention.output.dense.bias: frozen\n","encoder.layer.5.attention.output.dense.bias: frozen\n","encoder.layer.5.attention.output.dense.weight: frozen\n","encoder.layer.5.attention.output.dense.weight: frozen\n","encoder.layer.5.attention.self.key.bias: frozen\n","encoder.layer.5.attention.self.key.weight: frozen\n","encoder.layer.5.attention.self.query.bias: frozen\n","encoder.layer.5.attention.self.query.weight: frozen\n","encoder.layer.5.attention.self.value.bias: frozen\n","encoder.layer.5.attention.self.value.weight: frozen\n","encoder.layer.5.intermediate.dense.bias: frozen\n","encoder.layer.5.intermediate.dense.bias: frozen\n","encoder.layer.5.intermediate.dense.weight: frozen\n","encoder.layer.5.intermediate.dense.weight: frozen\n","encoder.layer.5.layernorm_after.bias: frozen\n","encoder.layer.5.layernorm_after.weight: frozen\n","encoder.layer.5.layernorm_before.bias: frozen\n","encoder.layer.5.layernorm_before.weight: frozen\n","encoder.layer.5.output.LayerNorm.bias: frozen\n","encoder.layer.5.output.LayerNorm.weight: frozen\n","encoder.layer.5.output.dense.bias: frozen\n","encoder.layer.5.output.dense.bias: frozen\n","encoder.layer.5.output.dense.weight: frozen\n","encoder.layer.5.output.dense.weight: frozen\n","encoder.layer.6.attention.attention.key.bias: frozen\n","encoder.layer.6.attention.attention.key.weight: frozen\n","encoder.layer.6.attention.attention.query.bias: frozen\n","encoder.layer.6.attention.attention.query.weight: frozen\n","encoder.layer.6.attention.attention.value.bias: frozen\n","encoder.layer.6.attention.attention.value.weight: frozen\n","encoder.layer.6.attention.output.LayerNorm.bias: frozen\n","encoder.layer.6.attention.output.LayerNorm.weight: frozen\n","encoder.layer.6.attention.output.dense.bias: frozen\n","encoder.layer.6.attention.output.dense.bias: frozen\n","encoder.layer.6.attention.output.dense.weight: frozen\n","encoder.layer.6.attention.output.dense.weight: frozen\n","encoder.layer.6.attention.self.key.bias: frozen\n","encoder.layer.6.attention.self.key.weight: frozen\n","encoder.layer.6.attention.self.query.bias: not frozen\n","encoder.layer.6.attention.self.query.weight: not frozen\n","encoder.layer.6.attention.self.value.bias: not frozen\n","encoder.layer.6.attention.self.value.weight: not frozen\n","encoder.layer.6.intermediate.dense.bias: not frozen\n","encoder.layer.6.intermediate.dense.bias: not frozen\n","encoder.layer.6.intermediate.dense.weight: not frozen\n","encoder.layer.6.intermediate.dense.weight: not frozen\n","encoder.layer.6.layernorm_after.bias: not frozen\n","encoder.layer.6.layernorm_after.weight: not frozen\n","encoder.layer.6.layernorm_before.bias: not frozen\n","encoder.layer.6.layernorm_before.weight: not frozen\n","encoder.layer.6.output.LayerNorm.bias: not frozen\n","encoder.layer.6.output.LayerNorm.weight: not frozen\n","encoder.layer.6.output.dense.bias: not frozen\n","encoder.layer.6.output.dense.bias: not frozen\n","encoder.layer.6.output.dense.weight: not frozen\n","encoder.layer.6.output.dense.weight: not frozen\n","encoder.layer.7.attention.attention.key.bias: not frozen\n","encoder.layer.7.attention.attention.key.weight: not frozen\n","encoder.layer.7.attention.attention.query.bias: not frozen\n","encoder.layer.7.attention.attention.query.weight: not frozen\n","encoder.layer.7.attention.attention.value.bias: not frozen\n","encoder.layer.7.attention.attention.value.weight: not frozen\n","encoder.layer.7.attention.output.LayerNorm.bias: not frozen\n","encoder.layer.7.attention.output.LayerNorm.weight: not frozen\n","encoder.layer.7.attention.output.dense.bias: not frozen\n","encoder.layer.7.attention.output.dense.bias: not frozen\n","encoder.layer.7.attention.output.dense.weight: not frozen\n","encoder.layer.7.attention.output.dense.weight: not frozen\n","encoder.layer.7.attention.self.key.bias: not frozen\n","encoder.layer.7.attention.self.key.weight: not frozen\n","encoder.layer.7.attention.self.query.bias: not frozen\n","encoder.layer.7.attention.self.query.weight: not frozen\n","encoder.layer.7.attention.self.value.bias: not frozen\n","encoder.layer.7.attention.self.value.weight: not frozen\n","encoder.layer.7.intermediate.dense.bias: not frozen\n","encoder.layer.7.intermediate.dense.bias: not frozen\n","encoder.layer.7.intermediate.dense.weight: not frozen\n","encoder.layer.7.intermediate.dense.weight: not frozen\n","encoder.layer.7.layernorm_after.bias: not frozen\n","encoder.layer.7.layernorm_after.weight: not frozen\n","encoder.layer.7.layernorm_before.bias: not frozen\n","encoder.layer.7.layernorm_before.weight: not frozen\n","encoder.layer.7.output.LayerNorm.bias: not frozen\n","encoder.layer.7.output.LayerNorm.weight: not frozen\n","encoder.layer.7.output.dense.bias: not frozen\n","encoder.layer.7.output.dense.bias: not frozen\n","encoder.layer.7.output.dense.weight: not frozen\n","encoder.layer.7.output.dense.weight: not frozen\n","encoder.layer.8.attention.attention.key.bias: not frozen\n","encoder.layer.8.attention.attention.key.weight: not frozen\n","encoder.layer.8.attention.attention.query.bias: not frozen\n","encoder.layer.8.attention.attention.query.weight: not frozen\n","encoder.layer.8.attention.attention.value.bias: not frozen\n","encoder.layer.8.attention.attention.value.weight: not frozen\n","encoder.layer.8.attention.output.LayerNorm.bias: not frozen\n","encoder.layer.8.attention.output.LayerNorm.weight: not frozen\n","encoder.layer.8.attention.output.dense.bias: not frozen\n","encoder.layer.8.attention.output.dense.bias: not frozen\n","encoder.layer.8.attention.output.dense.weight: not frozen\n","encoder.layer.8.attention.output.dense.weight: not frozen\n","encoder.layer.8.attention.self.key.bias: not frozen\n","encoder.layer.8.attention.self.key.weight: not frozen\n","encoder.layer.8.attention.self.query.bias: not frozen\n","encoder.layer.8.attention.self.query.weight: not frozen\n","encoder.layer.8.attention.self.value.bias: not frozen\n","encoder.layer.8.attention.self.value.weight: not frozen\n","encoder.layer.8.intermediate.dense.bias: not frozen\n","encoder.layer.8.intermediate.dense.bias: not frozen\n","encoder.layer.8.intermediate.dense.weight: not frozen\n","encoder.layer.8.intermediate.dense.weight: not frozen\n","encoder.layer.8.layernorm_after.bias: not frozen\n","encoder.layer.8.layernorm_after.weight: not frozen\n","encoder.layer.8.layernorm_before.bias: not frozen\n","encoder.layer.8.layernorm_before.weight: not frozen\n","encoder.layer.8.output.LayerNorm.bias: not frozen\n","encoder.layer.8.output.LayerNorm.weight: not frozen\n","encoder.layer.8.output.dense.bias: not frozen\n","encoder.layer.8.output.dense.bias: not frozen\n","encoder.layer.8.output.dense.weight: not frozen\n","encoder.layer.8.output.dense.weight: not frozen\n","encoder.layer.9.attention.attention.key.bias: not frozen\n","encoder.layer.9.attention.attention.key.weight: not frozen\n","encoder.layer.9.attention.attention.query.bias: not frozen\n","encoder.layer.9.attention.attention.query.weight: not frozen\n","encoder.layer.9.attention.attention.value.bias: not frozen\n","encoder.layer.9.attention.attention.value.weight: not frozen\n","encoder.layer.9.attention.output.LayerNorm.bias: not frozen\n","encoder.layer.9.attention.output.LayerNorm.weight: not frozen\n","encoder.layer.9.attention.output.dense.bias: not frozen\n","encoder.layer.9.attention.output.dense.bias: not frozen\n","encoder.layer.9.attention.output.dense.weight: not frozen\n","encoder.layer.9.attention.output.dense.weight: not frozen\n","encoder.layer.9.attention.self.key.bias: not frozen\n","encoder.layer.9.attention.self.key.weight: not frozen\n","encoder.layer.9.attention.self.query.bias: not frozen\n","encoder.layer.9.attention.self.query.weight: not frozen\n","encoder.layer.9.attention.self.value.bias: not frozen\n","encoder.layer.9.attention.self.value.weight: not frozen\n","encoder.layer.9.intermediate.dense.bias: not frozen\n","encoder.layer.9.intermediate.dense.bias: not frozen\n","encoder.layer.9.intermediate.dense.weight: not frozen\n","encoder.layer.9.intermediate.dense.weight: not frozen\n","encoder.layer.9.layernorm_after.bias: not frozen\n","encoder.layer.9.layernorm_after.weight: not frozen\n","encoder.layer.9.layernorm_before.bias: not frozen\n","encoder.layer.9.layernorm_before.weight: not frozen\n","encoder.layer.9.output.LayerNorm.bias: not frozen\n","encoder.layer.9.output.LayerNorm.weight: not frozen\n","encoder.layer.9.output.dense.bias: not frozen\n","encoder.layer.9.output.dense.bias: not frozen\n","encoder.layer.9.output.dense.weight: not frozen\n","encoder.layer.9.output.dense.weight: not frozen\n","layernorm.bias: not frozen\n","layernorm.weight: not frozen\n","pooler.dense.bias: not frozen\n","pooler.dense.bias: not frozen\n","pooler.dense.weight: not frozen\n","pooler.dense.weight: not frozen\n"]},{"data":{"text/plain":["DataParallel(\n","  (module): VisualQuestionAnsweringModel(\n","    (text_model): BertModel(\n","      (embeddings): BertEmbeddings(\n","        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","        (position_embeddings): Embedding(512, 768)\n","        (token_type_embeddings): Embedding(2, 768)\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (encoder): BertEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x BertLayer(\n","            (attention): BertAttention(\n","              (self): BertSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (output): BertSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","            (intermediate): BertIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): BertOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","        )\n","      )\n","      (pooler): BertPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (image_model): ViTModel(\n","      (embeddings): ViTEmbeddings(\n","        (patch_embeddings): ViTPatchEmbeddings(\n","          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n","        )\n","        (dropout): Dropout(p=0.0, inplace=False)\n","      )\n","      (encoder): ViTEncoder(\n","        (layer): ModuleList(\n","          (0-11): 12 x ViTLayer(\n","            (attention): ViTAttention(\n","              (attention): ViTSelfAttention(\n","                (query): Linear(in_features=768, out_features=768, bias=True)\n","                (key): Linear(in_features=768, out_features=768, bias=True)\n","                (value): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","              (output): ViTSelfOutput(\n","                (dense): Linear(in_features=768, out_features=768, bias=True)\n","                (dropout): Dropout(p=0.0, inplace=False)\n","              )\n","            )\n","            (intermediate): ViTIntermediate(\n","              (dense): Linear(in_features=768, out_features=3072, bias=True)\n","              (intermediate_act_fn): GELUActivation()\n","            )\n","            (output): ViTOutput(\n","              (dense): Linear(in_features=3072, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.0, inplace=False)\n","            )\n","            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          )\n","        )\n","      )\n","      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (pooler): ViTPooler(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (activation): Tanh()\n","      )\n","    )\n","    (classifier): Linear(in_features=1536, out_features=582, bias=True)\n","  )\n",")"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["model = VisualQuestionAnsweringModel(num_labels)\n","model.freeze_layers()\n","model = DataParallel(model)\n","model.to('cuda')"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T22:10:51.029760Z","iopub.status.busy":"2024-04-22T22:10:51.029151Z","iopub.status.idle":"2024-04-22T22:10:51.035196Z","shell.execute_reply":"2024-04-22T22:10:51.034186Z","shell.execute_reply.started":"2024-04-22T22:10:51.029730Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Loss over all epochs: 0.009664024118755535\n"]}],"source":["optimizer = Adam(model.parameters(), lr=5e-5)\n","criterion = CrossEntropyLoss()\n","num_epochs = 100\n","model.train() \n","for epoch in range(num_epochs):\n","    total_loss = 0\n","    for batch in train_loader:\n","        input_ids = batch['input_ids'].to('cuda')\n","        attention_mask = batch['attention_mask'].to('cuda')\n","        pixel_values = batch['pixel_values'].to('cuda')\n","        labels = batch['labels'].to('cuda')\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask, pixel_values)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    overall_loss += total_loss\n","print(f\"Total Loss over all epochs: {overall_loss}\")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T22:15:07.985572Z","iopub.status.busy":"2024-04-22T22:15:07.984575Z","iopub.status.idle":"2024-04-22T22:15:07.991390Z","shell.execute_reply":"2024-04-22T22:15:07.990410Z","shell.execute_reply.started":"2024-04-22T22:15:07.985535Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.13\n"]}],"source":["test_loader = DataLoader(VQADataset(dataset['test']), batch_size=32, shuffle=False)\n","model.eval()\n","torch.no_grad()\n","\n","predictions = []\n","true_labels = []\n","\n","with torch.no_grad(): \n","    for batch in test_loader:\n","        input_ids = batch['input_ids'].to('cuda')\n","        attention_mask = batch['attention_mask'].to('cuda')\n","        images = batch['pixel_values'].to('cuda')\n","        labels = batch['labels'].to('cuda')\n","        outputs = model(input_ids, attention_mask, images)\n","        preds = torch.argmax(outputs, dim=1)\n","        predictions.extend(preds.cpu().numpy())\n","        true_labels.extend(labels.cpu().numpy())\n","        \n","        \n","predicted_answers = [answer_space[pred] for pred in predictions]\n","true_answers = [answer_space[label] for label in true_labels]\n","\n","correct_count = sum(p == t for p, t in zip(predicted_answers, true_answers))\n","total = len(predicted_answers)\n","accuracy = correct_count / total\n","print(f\"Accuracy: {accuracy:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":2264789,"sourceId":3798293,"sourceType":"datasetVersion"}],"dockerImageVersionId":30703,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
